---
title: "chapter4"
author: "Dimitrios Tsallos"
date: "11/28/2017"
output: html_document
---


# 4^th^ Week

## Clustering and classification

### The Boston Dataset

This week we are using the Boston dataset as provided by the package MASS. [Harrison and Rubinfeld 1978](https://www.law.berkeley.edu/files/Hedonic.PDF) published this data for the first time but you can find more information, on a nice layout and simplyfied form, [here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html).


```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(tidyr)
library(dplyr)
# access the MASS package
library(MASS)

# load the data
data("Boston")

# explore the dataset
str(Boston)
dim(Boston)
summary(Boston)
```


From the data structure we can already see that most variables are numeric, the only variable that is integral is called chas and is a Charles River dummy variable (= 1 if tract bounds river; 0 otherwise). The following plots aim to help with the investigation of the data relationships. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}

gather(Boston) %>% 
  ggplot(aes(value)) + 
  geom_histogram() + stat_bin(bins = 30) + 
  facet_wrap("key", scales = "free") +
    theme_bw()
  
```





```{r, echo=FALSE}
# MASS, corrplot, tidyverse and Boston dataset are available
library(corrplot)

# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round( 2)

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type = "upper", tl.cex= 0.6)

```


So, let's standardise the dataset and see how it looks.
*Data have been standardised with this formula:* $$scaled(x)= \frac{x−mean(x)}{sd(x)} $$ 


```{r, echo=FALSE}

# center and standardize variables
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)

summary(boston_scaled)
```

To make it easier to work with, I am going to transform the crime variable into quantile bins and basically make it a categorical variable. 

```{r, echo=FALSE}
# summary of the scaled crime rate
summary(boston_scaled$crim)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c( "low", "med_low", "med_high", "high") )

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
str(boston_scaled)
```

Then, I am going to divide the dataset into two random groups, the train set and the test set. The train set consists of 80 % of the observations. 


```{r, message=FALSE, warning=FALSE, include=FALSE}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]
colnames(test)

# save the correct classes from test data
correct_classes <- test[, ncol(test)]

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

```



## Linear Discriminant analysis

Linear Discriminant analysis is a classification (and dimension reduction) method. It finds the (linear) combination of the variables that separate the target variable classes. The target can be binary or multiclass variable.

Linear discriminant analysis is closely related to many other methods, such as principal component analysis (we will look into that next week) and the already familiar logistic regression.

LDA can be visualized with a biplot. The LDA biplot arrow function used in the exercise is (with slight changes) taken from [this](https://stackoverflow.com/questions/17232251/how-can-i-plot-a-biplot-for-lda-in-r) Stack Overflow message thread.


```{r, echo=FALSE}
# linear discriminant analysis
lda.fit <- lda(crime ~., data = train)

# print the lda.fit object
lda.fit

```


Expectedly the prior probability of groups results to about 1 fourth in each case, since the classes created were based the data divided in quantiles. 
It seems that LD1 explains 94.5 % of the group variance, I will therefore use `dimen = 2´. 

```{r, echo=FALSE}

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)

```


Now, I can better understand the coefficients of linear discriminants based on the arrows on vusualised on the plot. It seems that the variable rad (index of accessibility to radial highways) has the biggest impact on crime (Coefficient of linear discriminants = 3.15 for LD1). Next, comes the variable zn (proportion of residential land zoned for lots over 25,000 sq.ft.) and nox (nitrogen oxides concentration (parts per 10 million)) that separate low from medium crime on a gradient. 

Let's see how is the model performing now. I am going to use the test dataset, the 20 % of the observations that were separated earlier from the main table. Is my model going to predict everything correctly?


```{r, echo=FALSE}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```



K-means 10 clusters works best here.

```{r, echo=FALSE}
library(MASS)
data("Boston")
boston_scaled2 <- scale(Boston)

# euclidean distance matrix
dist_eu <- dist(boston_scaled2, method = "euclidean")

# look at the summary of the distances
summary(dist_eu)

# k-means clustering
km <-kmeans(Boston, centers = 10)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)
```







## Bonus

Perform k-means on the original Boston data with some reasonable number of clusters (> 2). Remember to standardize the dataset. Then perform LDA using the clusters as target classes. Include all the variables in the Boston data in the LDA model. Visualize the results with a biplot (include arrows representing the relationships of the original variables to the LDA solution). Interpret the results. Which variables are the most influencial linear separators for the clusters? (0-2 points to compensate any loss of points from the above exercises)


```{r}
# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled2, k)$tot.withinss})

# k-means clustering
km <-kmeans(boston_scaled2, centers = 2)


lda.fit2 <- lda(crim ~., data = as.data.frame(boston_scaled2) )

# lda.fit2

# visualize the results
plot(lda.fit2, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit2, myscale = 2)

```

## Super-Bonus

Run the code below for the (scaled) train data that you used to fit the LDA. The code creates a matrix product, which is a projection of the data points.

```{r}
model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)
```


Next, install and access the plotly package. Create a 3D plot (Cool!) of the columns of the matrix product by typing the code below.



```{r}
library(plotly)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')

```


Adjust the code: add argument color as a argument in the plot_ly() function. Set the color to be the crime classes of the train set. Draw another 3D plot where the color is defined by the clusters of the k-means. How do the plots differ? Are there any similarities? (0-3 points to compensate any loss of points from the above exercises)

```{r}
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = train$crime )
```


```{r}
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color =  classes
            )
```

# bottom of the page